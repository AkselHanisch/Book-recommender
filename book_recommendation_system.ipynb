{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split \n",
    "from scipy.sparse.linalg import svds \n",
    "from sklearn import preprocessing \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lars\\AppData\\Local\\Temp\\ipykernel_25340\\2624249305.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_books=pd.read_csv(path +'filtered_books.csv')\n"
     ]
    }
   ],
   "source": [
    "path='./Data/'\n",
    "\n",
    "df_books=pd.read_csv(path +'filtered_books.csv') \n",
    "df_ratings=pd.read_csv(path + 'Ratings.csv') \n",
    "df_users=pd.read_csv(path + 'Users.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149780, 3)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>278858.00000</td>\n",
       "      <td>168096.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>139429.50000</td>\n",
       "      <td>34.751434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>80499.51502</td>\n",
       "      <td>14.428097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69715.25000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>139429.50000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>209143.75000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>278858.00000</td>\n",
       "      <td>244.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User-ID            Age\n",
       "count  278858.00000  168096.000000\n",
       "mean   139429.50000      34.751434\n",
       "std     80499.51502      14.428097\n",
       "min         1.00000       0.000000\n",
       "25%     69715.25000      24.000000\n",
       "50%    139429.50000      32.000000\n",
       "75%    209143.75000      44.000000\n",
       "max    278858.00000     244.000000"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851511, 12)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df=pd.merge(df_users,df_ratings,on='User-ID')\n",
    "merged_df=pd.merge(merged_df,df_books,on='ISBN')\n",
    "\n",
    "merged_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_review_counts = df_ratings['ISBN'].value_counts()\n",
    "popular_books = book_review_counts[book_review_counts >= 20].index\n",
    "filtered_ratings = df_ratings[df_ratings['ISBN'].isin(popular_books)]\n",
    "#filtered_ratings.to_csv('data/filtered_ratings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaberative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom sklearn.preprocessing import StandardScaler\\nfrom scipy.sparse import csr_matrix\\n\\n# Load the dataset\\ndata = pd.read_csv(\\'data/ratings.csv\\')\\ndata.fillna(0, inplace=True)\\n\\n\\n\\nbook_review_counts = data[\\'ISBN\\'].value_counts()\\npopular_books = book_review_counts[book_review_counts >= 20].index\\ndata = data[data[\\'ISBN\\'].isin(popular_books)]\\n\\n\\nuser_rating_counts = data[\\'User-ID\\'].value_counts()\\nactive_users = user_rating_counts[user_rating_counts >= 5].index\\ndata = data[data[\\'User-ID\\'].isin(active_users)]\\n\\n\\n\\n# Preprocess the data\\n\\n# Create a user-item matrix\\nuser_item_matrix = data.pivot(index=\\'User-ID\\', columns=\\'ISBN\\', values=\\'Book-Rating\\').fillna(0)\\n\\n# Convert the user-item matrix to a sparse matrix\\nuser_item_matrix_sparse = csr_matrix(user_item_matrix.values).astype(\\'float32\\')\\n\\n# Normalize the user-item matrix\\nscaler = StandardScaler(with_mean=False)\\nuser_item_matrix_normalized = scaler.fit_transform(user_item_matrix_sparse)\\n\\n\\n# Compute user-user similarity\\n\\n\\nuser_similarity = cosine_similarity(user_item_matrix_normalized)\\n\\n\\n# Convert the similarity matrix back to a DataFrame\\nuser_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\\n\\n\\n# Function to get top N similar users\\ndef get_top_n_similar_users(user_id, n):\\n    similar_users = user_similarity_df[user_id].sort_values(ascending=False).head(n+1).index.tolist()\\n    print(\"similar users: \",similar_users)\\n    similar_users.remove(user_id) # Remove the user_id itself from the list\\n    return similar_users\\n\\n# Function to generate recommendations for a user\\ndef recommend_items(user_id, n):\\n    similar_users = get_top_n_similar_users(user_id, n)\\n    similar_users_ratings = user_item_matrix.loc[similar_users]\\n\\n    user_similarities = user_similarity_df.loc[user_id, similar_users]\\n\\n    # Calculate weighted scores\\n    weighted_scores = similar_users_ratings.T.dot(user_similarities).div(user_similarities.sum())\\n    \\n    # Ignore items already rated by the target user\\n    target_user_ratings = user_item_matrix.loc[user_id]\\n    weighted_scores[target_user_ratings > 0] = 0\\n    \\n    # Return the top-N recommendations\\n    return weighted_scores.sort_values(ascending=False).head(n)\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/ratings.csv')\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "book_review_counts = data['ISBN'].value_counts()\n",
    "popular_books = book_review_counts[book_review_counts >= 20].index\n",
    "data = data[data['ISBN'].isin(popular_books)]\n",
    "\n",
    "\n",
    "user_rating_counts = data['User-ID'].value_counts()\n",
    "active_users = user_rating_counts[user_rating_counts >= 5].index\n",
    "data = data[data['User-ID'].isin(active_users)]\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "# Create a user-item matrix\n",
    "user_item_matrix = data.pivot(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)\n",
    "\n",
    "# Convert the user-item matrix to a sparse matrix\n",
    "user_item_matrix_sparse = csr_matrix(user_item_matrix.values).astype('float32')\n",
    "\n",
    "# Normalize the user-item matrix\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "user_item_matrix_normalized = scaler.fit_transform(user_item_matrix_sparse)\n",
    "\n",
    "\n",
    "# Compute user-user similarity\n",
    "\n",
    "\n",
    "user_similarity = cosine_similarity(user_item_matrix_normalized)\n",
    "\n",
    "\n",
    "# Convert the similarity matrix back to a DataFrame\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "\n",
    "\n",
    "# Function to get top N similar users\n",
    "def get_top_n_similar_users(user_id, n):\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False).head(n+1).index.tolist()\n",
    "    print(\"similar users: \",similar_users)\n",
    "    similar_users.remove(user_id) # Remove the user_id itself from the list\n",
    "    return similar_users\n",
    "\n",
    "# Function to generate recommendations for a user\n",
    "def recommend_items(user_id, n):\n",
    "    similar_users = get_top_n_similar_users(user_id, n)\n",
    "    similar_users_ratings = user_item_matrix.loc[similar_users]\n",
    "\n",
    "    user_similarities = user_similarity_df.loc[user_id, similar_users]\n",
    "\n",
    "    # Calculate weighted scores\n",
    "    weighted_scores = similar_users_ratings.T.dot(user_similarities).div(user_similarities.sum())\n",
    "    \n",
    "    # Ignore items already rated by the target user\n",
    "    target_user_ratings = user_item_matrix.loc[user_id]\n",
    "    weighted_scores[target_user_ratings > 0] = 0\n",
    "    \n",
    "    # Return the top-N recommendations\n",
    "    return weighted_scores.sort_values(ascending=False).head(n)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage\\n#print(user_similarity_df.head)\\nuser_id = 121138\\nrecommendations = recommend_items(user_id, 20)\\nprint(f\"Recommendations for user {user_id}: {recommendations}\")\\n\\n#user_similarity_df.shape\\n#user_similarity_df.head()\\n'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Example usage\n",
    "#print(user_similarity_df.head)\n",
    "user_id = 121138\n",
    "recommendations = recommend_items(user_id, 20)\n",
    "print(f\"Recommendations for user {user_id}: {recommendations}\")\n",
    "\n",
    "#user_similarity_df.shape\n",
    "#user_similarity_df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbook_review_counts = df_ratings[\\'ISBN\\'].value_counts()\\npopular_books = book_review_counts[book_review_counts >= 20].index\\nfiltered_ratings = df_ratings[df_ratings[\\'ISBN\\'].isin(popular_books)]\\n\\nuser_rating_counts = filtered_ratings[\\'User-ID\\'].value_counts()\\nactive_users = user_rating_counts[user_rating_counts >= 5].index\\nfiltered_ratings = filtered_ratings[filtered_ratings[\\'User-ID\\'].isin(active_users)]\\n\\n\\nuser_item_matrix = filtered_ratings.pivot(index=\\'User-ID\\', columns=\\'ISBN\\', values=\\'Book-Rating\\').fillna(0)\\n\\n\\n\\nsparse_matrix = csr_matrix(user_item_matrix.values)\\nsparse_matrix=sparse_matrix.astype(\\'float32\\')\\n\\n\\nitem_similarity = cosine_similarity(sparse_matrix.T)  # Transpose for item-item similarity\\nitem_similarity_df = pd.DataFrame(\\n    item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns\\n)\\n\\nprint(\"shape: \",item_similarity_df.shape)\\n\\n\\n\\n# Step 4: Define a function to predict ratings\\ndef predict_ratings(user_id, user_item_matrix, item_similarity_df, k=5):\\n    \\n    user_ratings = user_item_matrix.loc[user_id]\\n    \\n    # Align user\\'s ratings with item_similarity_df index (ISBNs)\\n    user_ratings = user_ratings.reindex(item_similarity_df.index, fill_value=0)\\n    \\n    #print(\"User ratings\", user_item_matrix.shape)\\n    \\n    # Compute weighted average of item similarities and user ratings\\n    scores = item_similarity_df.dot(user_ratings)\\n    \\n    \\n    scores[user_ratings > 0] = 0\\n\\n    \\n    #print(scores)\\n    \\n    # Return top-k recommendations\\n    return scores.sort_values(ascending=False).head(k)\\n\\n\\n\\nexample_user_id = 165  # Choose the first user as an example\\nrecommendations = predict_ratings(example_user_id, user_item_matrix, item_similarity_df, k=20)\\n\\nrecommendations\\n\\n'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "book_review_counts = df_ratings['ISBN'].value_counts()\n",
    "popular_books = book_review_counts[book_review_counts >= 20].index\n",
    "filtered_ratings = df_ratings[df_ratings['ISBN'].isin(popular_books)]\n",
    "\n",
    "user_rating_counts = filtered_ratings['User-ID'].value_counts()\n",
    "active_users = user_rating_counts[user_rating_counts >= 5].index\n",
    "filtered_ratings = filtered_ratings[filtered_ratings['User-ID'].isin(active_users)]\n",
    "\n",
    "\n",
    "user_item_matrix = filtered_ratings.pivot(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "sparse_matrix = csr_matrix(user_item_matrix.values)\n",
    "sparse_matrix=sparse_matrix.astype('float32')\n",
    "\n",
    "\n",
    "item_similarity = cosine_similarity(sparse_matrix.T)  # Transpose for item-item similarity\n",
    "item_similarity_df = pd.DataFrame(\n",
    "    item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns\n",
    ")\n",
    "\n",
    "print(\"shape: \",item_similarity_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Define a function to predict ratings\n",
    "def predict_ratings(user_id, user_item_matrix, item_similarity_df, k=5):\n",
    "    \n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    \n",
    "    # Align user's ratings with item_similarity_df index (ISBNs)\n",
    "    user_ratings = user_ratings.reindex(item_similarity_df.index, fill_value=0)\n",
    "    \n",
    "    #print(\"User ratings\", user_item_matrix.shape)\n",
    "    \n",
    "    # Compute weighted average of item similarities and user ratings\n",
    "    scores = item_similarity_df.dot(user_ratings)\n",
    "    \n",
    "    \n",
    "    scores[user_ratings > 0] = 0\n",
    "\n",
    "    \n",
    "    #print(scores)\n",
    "    \n",
    "    # Return top-k recommendations\n",
    "    return scores.sort_values(ascending=False).head(k)\n",
    "\n",
    "\n",
    "\n",
    "example_user_id = 165  # Choose the first user as an example\n",
    "recommendations = predict_ratings(example_user_id, user_item_matrix, item_similarity_df, k=20)\n",
    "\n",
    "recommendations\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('0768322413', 10), ('0345362721', 9), ('0679408835', 8), ('0446517909', 7), ('0590257889', 6), ('0553802542', 5), ('0802132898', 4), ('0385491050', 3), ('0140042393', 2), ('015610685X', 1), ('0786003677', 10), ('0671870602', 9), ('0671685112', 8), ('0380792923', 7), ('1572460733', 6), ('0750925493', 5), ('0714530387', 4), ('0743467175', 3), ('1582430438', 2), ('0743418700', 1)])\n",
      "['0768322413', '0786003677', '0345362721', '0671870602', '0679408835', '0671685112', '0446517909', '0380792923', '0590257889', '1572460733']\n",
      "Content-Based Recommendations for user 165:\n",
      "0768322413\n",
      "0786003677\n",
      "0345362721\n",
      "0671870602\n",
      "0679408835\n",
      "0671685112\n",
      "0446517909\n",
      "0380792923\n",
      "0590257889\n",
      "1572460733\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from NLP_TFIDF import get_recommendations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "df_books = pd.read_csv('merged_books.csv')\n",
    "df_ratings = pd.read_csv('Data/Ratings.csv')\n",
    "\n",
    "# Filter the ratings dataset to include only the books present in merged_books.csv\n",
    "filtered_ratings = df_ratings[df_ratings['ISBN'].isin(df_books['ISBN'])]\n",
    "\n",
    "# Save the filtered dataset to a new CSV file\n",
    "filtered_ratings.to_csv('filtered_ratings.csv', index=False)\n",
    "\n",
    "# Reload the books dataset\n",
    "df_books = pd.read_csv('merged_books.csv')\n",
    "\n",
    "# Compute TF-IDF matrix for content-based filtering\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_books['description'].fillna(''))\n",
    "\n",
    "# Function to get the title of the recommended books\n",
    "def get_title(book_id):\n",
    "    if book_id in df_books['ISBN'].values:\n",
    "        return df_books[df_books['ISBN'] == book_id]['Book-Title'].values[0]\n",
    "    return None\n",
    "\n",
    "# NLP-TFIDF Only Recommendation Function\n",
    "def get_nlp_tfidf_recommendations(title, k=10):\n",
    "    # Get content-based recommendations\n",
    "    content_based_recommendations = get_recommendations(title)\n",
    "    \n",
    "    # Ensure we have at least k recommendations\n",
    "    recommendations = content_based_recommendations[:k]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Function to get recommendations based on all books a user has read\n",
    "def get_user_based_nlp_tfidf_recommendations(user_id, k=10):\n",
    "    # Get the list of books the user has read\n",
    "    user_books = filtered_ratings[filtered_ratings['User-ID'] == user_id]['ISBN'].unique()\n",
    "    \n",
    "    # Dictionary to store recommendations and their scores\n",
    "    recommendation_scores = {}\n",
    "    \n",
    "    # Get recommendations for each book the user has read\n",
    "    for book_id in user_books:\n",
    "        book_title = get_title(book_id)\n",
    "        if book_title:\n",
    "            recommendations = get_nlp_tfidf_recommendations(book_title, k)\n",
    "            for i, rec in enumerate(recommendations):\n",
    "                score = k - i  # Assign scores from k to 1\n",
    "                rec = df_books[df_books['Book-Title'] == rec]['ISBN'].values[0]\n",
    "\n",
    "                if rec in recommendation_scores:\n",
    "                    recommendation_scores[rec] += score\n",
    "                else:\n",
    "                    recommendation_scores[rec] = score\n",
    "    \n",
    "    print(recommendation_scores.items())\n",
    "    # Sort recommendations by their scores\n",
    "    sorted_recommendations = sorted(recommendation_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Ensure we have at least k recommendations\n",
    "    final_recommendations = [rec for rec, score in sorted_recommendations[:k]]\n",
    " \n",
    "\n",
    "    return final_recommendations\n",
    "\n",
    "# Example usage\n",
    "user_id = 165\n",
    "book_title = \"Destiny\"\n",
    "recommendations = get_user_based_nlp_tfidf_recommendations(user_id, k=10)\n",
    "print(recommendations)\n",
    "print(f\"Content-Based Recommendations for user {user_id}:\")\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_nlp_recommendations_by_user_program' from 'nlp_based' (c:\\Users\\Lars\\repos\\school projects\\H24\\Book-recommender\\nlp_based.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[342], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontent_based_file\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_content_based_recommendations_by_user\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muser_based_file\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_user_based_recommendations_by_user\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlp_based\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_nlp_recommendations_by_user_program\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Variables for content based filtering\u001b[39;00m\n\u001b[0;32m      9\u001b[0m book_review_counts \u001b[38;5;241m=\u001b[39m df_ratings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISBN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_nlp_recommendations_by_user_program' from 'nlp_based' (c:\\Users\\Lars\\repos\\school projects\\H24\\Book-recommender\\nlp_based.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from content_based_file import get_content_based_recommendations_by_user\n",
    "from user_based_file import get_user_based_recommendations_by_user\n",
    "from nlp_based_file import get_nlp_recommendations_by_user_program\n",
    "\n",
    "\n",
    "#Variables for content based filtering\n",
    "\n",
    "book_review_counts = df_ratings['ISBN'].value_counts()\n",
    "popular_books = book_review_counts[book_review_counts >= 20].index\n",
    "filtered_ratings = df_ratings[df_ratings['ISBN'].isin(popular_books)]\n",
    "\n",
    "user_rating_counts = filtered_ratings['User-ID'].value_counts()\n",
    "active_users = user_rating_counts[user_rating_counts >= 5].index\n",
    "filtered_ratings = filtered_ratings[filtered_ratings['User-ID'].isin(active_users)]\n",
    "\n",
    "\n",
    "user_item_matrix = filtered_ratings.pivot(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "sparse_matrix = csr_matrix(user_item_matrix.values)\n",
    "sparse_matrix=sparse_matrix.astype('float32')\n",
    "\n",
    "\n",
    "item_similarity = cosine_similarity(sparse_matrix.T)  # Transpose for item-item similarity\n",
    "item_similarity_df = pd.DataFrame(\n",
    "    item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#print(testfunction)\n",
    "\n",
    "\n",
    "\n",
    "example_user_id = 165  # Choose the first user as an example\n",
    "recommendations = get_content_based_recommendations_by_user(example_user_id, user_item_matrix, item_similarity_df, 10)\n",
    "\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar users:  [165, 136733, 50711, 275610, 96354, 122235, 218836, 207727, 130793, 212009, 33036, 144348, 101299, 78545, 222941, 113618, 37790, 21870, 243607, 191913, 42759, 81854, 486, 28709, 257804, 112818, 96589, 216336, 248850, 112598, 194735, 66591, 250196, 93421, 110493, 189891, 6501, 147687, 82901, 65653, 231694, 179591, 242878, 110165, 80036, 143807, 136104, 266697, 46197, 185468, 55421]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_nlp_recommendations_by_user_program' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[328], line 91\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m final_recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_user_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43muser_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnlp_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n_recommendations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_recommendations)\n",
      "Cell \u001b[1;32mIn[328], line 60\u001b[0m, in \u001b[0;36mcombine_recommendations\u001b[1;34m(user_id, user_weight, content_weight, nlp_weight, n)\u001b[0m\n\u001b[0;32m     58\u001b[0m user_based_recs \u001b[38;5;241m=\u001b[39m get_user_based_recommendations(user_id, n)\n\u001b[0;32m     59\u001b[0m content_based_recs \u001b[38;5;241m=\u001b[39m get_content_based_recommendations(user_id, n)\n\u001b[1;32m---> 60\u001b[0m nlp_based_recs\u001b[38;5;241m=\u001b[39m\u001b[43mget_nlp_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m  \u001b[38;5;66;03m# Scale scores\u001b[39;00m\n\u001b[0;32m     63\u001b[0m user_based_recs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scale_scores(user_based_recs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_score\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[328], line 39\u001b[0m, in \u001b[0;36mget_nlp_recommendations\u001b[1;34m(user_id)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nlp_recommendations\u001b[39m(user_id):\n\u001b[0;32m     38\u001b[0m     n\u001b[38;5;241m=\u001b[39mbooks_read_by_user_with_description(user_id)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 39\u001b[0m     recommended_items \u001b[38;5;241m=\u001b[39m \u001b[43mget_nlp_recommendations_by_user_program\u001b[49m(user_id, n)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISBN\u001b[39m\u001b[38;5;124m'\u001b[39m: recommended_items,\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(n, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assign descending scores\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_nlp_recommendations_by_user_program' is not defined"
     ]
    }
   ],
   "source": [
    "user_weight = 0.3\n",
    "content_weight = 0.35\n",
    "nlp_weight=0.35\n",
    "top_n_recommendations = 50\n",
    "\n",
    "example_user_id = 165\n",
    "\n",
    "\n",
    "\n",
    "def scale_scores(scores, scale_to=10):\n",
    "    max_score = scores.max()\n",
    "    if max_score > 0:  # Avoid division by zero\n",
    "        return (scores / max_score) * scale_to\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "# User-based recommendations\n",
    "def get_user_based_recommendations(user_id, n=top_n_recommendations):\n",
    "    recommended_items = get_user_based_recommendations_by_user(user_id, n)\n",
    "    return pd.DataFrame({\n",
    "        'ISBN': recommended_items.index,\n",
    "        'user_score': recommended_items.values\n",
    "    })\n",
    "\n",
    "# Content-based recommendations\n",
    "def get_content_based_recommendations(user_id, n=top_n_recommendations):\n",
    "    recommended_items = get_content_based_recommendations_by_user(user_id, user_item_matrix, item_similarity_df, k=n)\n",
    "    return pd.DataFrame({\n",
    "        'ISBN': recommended_items.index,\n",
    "        'content_score': recommended_items.values\n",
    "    })\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def get_nlp_recommendations(user_id):\n",
    "    n=books_read_by_user_with_description(user_id)*10\n",
    "    recommended_items = get_nlp_recommendations_by_user_program(user_id, n)\n",
    "    return pd.DataFrame({\n",
    "        'ISBN': recommended_items,\n",
    "        'nlp_score': range(n, 0, -1)  # Assign descending scores\n",
    "    })\n",
    "    \n",
    "\n",
    "    \n",
    "def books_read_by_user_with_description(user_id):\n",
    "        user_books = filtered_ratings[filtered_ratings['User-ID'] == user_id]['ISBN'].unique()\n",
    "        return len(user_books)\n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "# Combine the three methods\n",
    "def combine_recommendations(user_id, user_weight, content_weight,nlp_weight, n=top_n_recommendations):\n",
    "    # Get top recommendations from both methods\n",
    "    user_based_recs = get_user_based_recommendations(user_id, n)\n",
    "    content_based_recs = get_content_based_recommendations(user_id, n)\n",
    "    nlp_based_recs=get_nlp_recommendations(user_id)\n",
    "    \n",
    "     # Scale scores\n",
    "    user_based_recs['user_score'] = scale_scores(user_based_recs['user_score'])\n",
    "    content_based_recs['content_score'] = scale_scores(content_based_recs['content_score'])\n",
    "    nlp_based_recs['nlp_score']=scale_scores(nlp_based_recs['nlp_score'])\n",
    "    \n",
    "    # Merge on ISBN\n",
    "    combined = pd.merge(user_based_recs, content_based_recs, on='ISBN', how='outer')\n",
    "    combined = pd.merge(combined, nlp_based_recs, on='ISBN', how='outer')\n",
    "\n",
    "    \n",
    "    # Fill missing scores with 0\n",
    "    combined['user_score'] = combined['user_score'].fillna(0)\n",
    "    combined['content_score'] = combined['content_score'].fillna(0)\n",
    "    combined['nlp_score']=combined['nlp_score'].fillna(0)\n",
    "    \n",
    "    # Calculate hybrid score\n",
    "    combined['hybrid_score'] = (user_weight * combined['user_score'] +\n",
    "                                content_weight * combined['content_score']+\n",
    "                                nlp_weight*combined['nlp_score'])\n",
    "    \n",
    "    # Sort by hybrid score\n",
    "    combined = combined.sort_values(by='hybrid_score', ascending=False)\n",
    "    \n",
    "    # Return top recommendations\n",
    "    return combined.head(40)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "final_recommendations = combine_recommendations(example_user_id,user_weight , content_weight,nlp_weight ,n=top_n_recommendations)\n",
    "\n",
    "print(final_recommendations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
